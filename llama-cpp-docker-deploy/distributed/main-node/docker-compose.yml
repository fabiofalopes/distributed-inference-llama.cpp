version: '3.8'

services:
  llama-server:
    build:
      context: .
      dockerfile: llama-server.Dockerfile
    image: llama-cpp-main-server
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models
    environment:
      - MODEL=/app/models/${MODEL_DIR}/${MODEL_FILE}
      - RPC_SERVERS=${WORKER_IPS:-worker1_ip:50052,worker2_ip:50052}
      - N_GPU_LAYERS=${N_GPU_LAYERS:-99}
      - HOST=0.0.0.0
      - PORT=8080
      - CTX_SIZE=${CTX_SIZE:-2048}
      - ALIAS=${MODEL_ALIAS:-llama-model}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu] 