# Model configuration
MODEL_FILE=DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf
MODEL_ALIAS=deepseek-llama-8b
N_GPU_LAYERS=99
CTX_SIZE=4096  # Example: Adjust for your specific model! # IMPORTANT: Set this to the model's trained context size!
N_GQA=8  #  Important for some models, check model card.

# Server configuration
HOST=0.0.0.0
PORT=8080
CUDA_ARCHITECTURES=86 